---
title: "Homework 3"
author: "Lunbei Hu"
date: '`r format(Sys.time(), "%Y-%m-%d")`'
output: github_document
---

This is my solution to HW2.

```{r}
library(tidyverse)
library(patchwork)
```

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.color = "viridis",
  ggplot2.continuous.fill = "viridis"
)  

scale_colour_discrete = scale_color_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

### Problem 1

Load the instacart data.

```{r}
library(p8105.datasets)
data("instacart")
```

This dataset contains `r nrow(instacart)` rows and `r ncol(instacart)` columns.

Observations are the level of items in orders by user. There are user/order variables -- user ID, order ID, order day and order hour. There are also item variables -- name, aisle, department, and some numeric codes.

How many aisles, and which are most items from?

```{r}
instacart %>% 
  count(aisle) %>% 
  arrange(desc(n))
```


Let's make a plot

```{r}
instacart %>% 
  count(aisle) %>% 
  filter(n > 10000) %>% 
  mutate(
    aisle = factor(aisle),
    aisle = fct_reorder(aisle, n)
  ) %>% 
  ggplot(aes(x = aisle, y = n)) +
  geom_point() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```



Let's make a table!!

```{r}
instacart %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
  group_by(aisle) %>% 
  count(product_name) %>% 
  mutate(rank = min_rank(desc(n))) %>% 
  filter(rank < 4) %>% 
  arrange(aisle, rank) %>%
  knitr::kable()
```


Apples vs ice cream.

```{r}
instacart %>% 
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  group_by(product_name, order_dow) %>% 
  summarize(mean_hour = mean(order_hour_of_day)) %>% 
  pivot_wider(
    names_from = order_dow,
    values_from = mean_hour
  )
```


### Problem 2

Load the accelerometers data.

```{r}
accel_df =
  read.csv("./data/accel_data.csv") %>% 
  janitor::clean_names() %>% 
  pivot_longer(
    activity_1:activity_1440,
    names_to = "day_minute",
    names_prefix = "activity_",
    values_to = "activity_count"
  ) %>% 
  mutate(
    day = factor(day),
    day = forcats::fct_relevel(day, c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday")),
    week_day = case_when(
      day %in% c("Monday","Tuesday","Wednesday","Thursday","Friday") ~ "weekday",
      day %in% c("Saturday","Sunday") ~ "weekend"
    )
  )
```

There are `r nrow(accel_df)` observations and `r ncol(accel_df)`variables.

Create a table on the total activity over the day.

```{r}
accel_df %>% 
  group_by(week,day) %>% 
  summarize(total_activity = sum(activity_count)) %>% 
  pivot_wider(
    names_from = day,
    values_from = total_activity
  ) %>% 
  knitr::kable()
```

For the trend, Fridays tend to have the longest duration of activity.
For week 4 and 5, it looks like Saturdays there is no activity.

Make a single-panel plot that shows the 24-hour activity time courses for each day.

```{r}
accel_df %>% 
  mutate(day_minute = as.numeric(day_minute)) %>% 
  ggplot(aes(x = day_minute, y = activity_count, group = day_id, color = day)) +
  geom_line(alpha = .2) +
  geom_smooth(aes(group = day)) 
```

In Sundays, activity time tends to be between 500 to 750 minute. In Fridays, Saturday, and Mondays, activity time tends to be between 1250 and 1500 minutes.

## Question 3

Load the NY NOAA data.
```{r}
library(p8105.datasets)
data("ny_noaa")
```

Data cleaning

```{r}
ny_noaa_df = ny_noaa %>% 
  separate(date, into = c("year", "month", "day"), convert = TRUE) %>% 
  mutate(
    prcp = as.numeric(prcp),
    tmax = as.numeric(tmax),
    tmin = as.numeric(tmin),
    month = month.abb[as.factor(month)],
    prcp = prcp / 10,
    tmax = tmax / 10,
    tmin = tmin / 10
  )
```

Make a two-panel plot showing the average max temperature in January and in July in each station across years.

```{r}
ny_noaa_df %>% 
  filter(month == "Jan" | month == "Jul") %>% 
  group_by(id, year, month) %>% 
  summarize(avg_tmax = mean(tmax, na.rm = TRUE)) %>% 
  ggplot(aes(x = year, y = avg_tmax, group = id)) +
  geom_point() +
  geom_path(alpha = .05, size = .1) +
  facet_grid(. ~ month)
```

Make a two-panel plot showing:

(i) tmax vs tmin for the full dataset

```{r}
tmax_tmin_p = 
  ny_noaa_df %>% 
  ggplot(aes(x = tmin, y = tmax)) +
  geom_hex()
```

Generally, Jan has lower tmax than July.
And there are 3 outliers in Jan and July in the lower range of the tmax, separately.

(ii) the distribution of snowfall values greater than 0 and less than 100 separately by year

```{r}
snow_p =
  ny_noaa_df %>% 
  drop_na(snow) %>% 
  filter(snow > 0 & snow < 100) %>% 
  ggplot(aes(x = year, y = snow, fill = year)) +
  geom_violin() 

tmax_tmin_p + snow_p
```

